{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c1f2f0db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from data import CIFData, collate_pool, get_train_val_test_loader\n",
    "from model import CrystalGraphConvNet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad1d4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(123)\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e4f6996",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CIFData(root_dir='F:\\College\\Research Paper Work\\Superconductor\\models\\CGCNN\\data', max_num_nbr=20, radius=15,\n",
    "                  dmin=0, step=0.2, random_seed=123)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47345e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    batch_atom_fea = []\n",
    "    batch_nbr_fea = []\n",
    "    batch_nbr_fea_idx = []\n",
    "    batch_crystal_atom_idx = []\n",
    "    batch_target = []\n",
    "    batch_cif_ids = []\n",
    "\n",
    "    base_idx = 0\n",
    "    for (atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx), target, cif_id in batch:\n",
    "        n_i = atom_fea.shape[0]\n",
    "\n",
    "        batch_atom_fea.append(atom_fea)\n",
    "        batch_nbr_fea.append(nbr_fea)\n",
    "        batch_nbr_fea_idx.append(nbr_fea_idx)\n",
    "        batch_crystal_atom_idx.extend([i + base_idx for i in crystal_atom_idx])\n",
    "        batch_target.append(target.view(1))  # ensure shape [1]\n",
    "        batch_cif_ids.append(cif_id)\n",
    "\n",
    "        base_idx += n_i\n",
    "\n",
    "    return (\n",
    "        torch.cat(batch_atom_fea, dim=0),              # [total_atoms, atom_fea_len]\n",
    "        torch.cat(batch_nbr_fea, dim=0),               # [total_atoms, max_num_nbr, nbr_fea_len]\n",
    "        torch.cat(batch_nbr_fea_idx, dim=0),           # [total_atoms, max_num_nbr]\n",
    "        torch.LongTensor(batch_crystal_atom_idx),      # [total_neighbors]\n",
    "    ),  torch.tensor(batch_target, dtype=torch.float), batch_cif_ids   # targets as [batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5647a681",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ratio = 0.8\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.1\n",
    "batch_size = 64\n",
    "num_workers = 4\n",
    "pin_memory = True  # or False depending on your CUDA usage\n",
    "\n",
    "# Use ratio-based split (can also specify fixed sizes instead)\n",
    "train_loader, val_loader, test_loader = get_train_val_test_loader(\n",
    "    dataset=dataset,\n",
    "    collate_fn=collate_batch, \n",
    "    batch_size=1,\n",
    "    train_ratio=train_ratio,\n",
    "    val_ratio=val_ratio,\n",
    "    test_ratio=test_ratio,\n",
    "    num_workers=0,\n",
    "    pin_memory=pin_memory,\n",
    "    train_size=None,  # or set to exact int like 4618 (0.8 * 5773)\n",
    "    val_size=None,\n",
    "    test_size=None,\n",
    "    return_test=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0c7e28c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CrystalGraphConvNet(\n",
       "  (embedding): Linear(in_features=4, out_features=64, bias=True)\n",
       "  (convs): ModuleList(\n",
       "    (0-2): 3 x ConvLayer(\n",
       "      (fc_full): Linear(in_features=131, out_features=128, bias=True)\n",
       "      (sigmoid): Sigmoid()\n",
       "      (softplus1): Softplus(beta=1.0, threshold=20.0)\n",
       "      (bn1): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (bn2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (softplus2): Softplus(beta=1.0, threshold=20.0)\n",
       "    )\n",
       "  )\n",
       "  (conv_to_fc): Linear(in_features=64, out_features=128, bias=True)\n",
       "  (conv_to_fc_softplus): Softplus(beta=1.0, threshold=20.0)\n",
       "  (fc_out): Linear(in_features=128, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from model import CrystalGraphConvNet\n",
    "\n",
    "model = CrystalGraphConvNet(\n",
    "    orig_atom_fea_len=4,\n",
    "    nbr_fea_len=3,\n",
    "    atom_fea_len=64,\n",
    "    n_conv=3,\n",
    "    h_fea_len=128,\n",
    "    n_h=1,\n",
    "    classification=False  # change to True if youâ€™re doing classification\n",
    ")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3803645f",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()  # for regression tasks\n",
    "optimizer = Adam(model.parameters(), lr=1e-3, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "52cb19a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for (atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx), target, cif_id in train_loader:\n",
    "        # Move tensors to device\n",
    "        atom_fea = atom_fea.to(device)\n",
    "        nbr_fea = nbr_fea.to(device)\n",
    "        nbr_fea_idx = nbr_fea_idx.to(device)\n",
    "        crystal_atom_idx = crystal_atom_idx.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        # Print tensor shapes for debugging\n",
    "        print(\"atom_fea:\", atom_fea.shape)\n",
    "        print(\"nbr_fea:\", nbr_fea.shape)\n",
    "        print(\"nbr_fea_idx:\", nbr_fea_idx.shape)\n",
    "        print(\"crystal_atom_idx:\", crystal_atom_idx.shape)\n",
    "        print(\"target:\", target.shape)\n",
    "\n",
    "        # Flatten target to shape [batch_size]\n",
    "        target = target.view(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "        print(\"Model output shape:\",output.shape)\n",
    "\n",
    "        # Flatten output to match target shape\n",
    "        output = output.view(-1)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for (atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx), target, _ in val_loader:\n",
    "            atom_fea, nbr_fea, nbr_fea_idx = atom_fea.to(device), nbr_fea.to(device), nbr_fea_idx.to(device)\n",
    "            target = target.to(device).float()\n",
    "            output = model(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item() * target.size(0)\n",
    "    return total_loss / len(val_loader.dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "56dda0b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n",
      "atom_fea: torch.Size([13, 4])\n",
      "nbr_fea: torch.Size([13, 20, 3])\n",
      "nbr_fea_idx: torch.Size([13, 20])\n",
      "crystal_atom_idx: torch.Size([260])\n",
      "target: torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\saman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pymatgen\\core\\structure.py:3102: UserWarning: Issues encountered while parsing CIF: 8 fractional coordinates rounded to ideal values to avoid issues with finite precision.\n",
      "  struct = parser.parse_structures(primitive=primitive)[0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "len() of a 0-d tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[32m      8\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     train_loss = \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m     val_loss, all_preds, all_targets = validate(model, val_loader, criterion, device, return_preds=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     13\u001b[39m     r2 = r2_score(all_targets, all_preds)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 24\u001b[39m, in \u001b[36mtrain_one_epoch\u001b[39m\u001b[34m(model, train_loader, criterion, optimizer, device)\u001b[39m\n\u001b[32m     21\u001b[39m target = target.view(-\u001b[32m1\u001b[39m)\n\u001b[32m     23\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43matom_fea\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbr_fea\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbr_fea_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrystal_atom_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel output shape:\u001b[39m\u001b[33m\"\u001b[39m,output.shape)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Flatten output to match target shape\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\College\\Research Paper Work\\Superconductor\\models\\CGCNN\\train\\model.py:155\u001b[39m, in \u001b[36mCrystalGraphConvNet.forward\u001b[39m\u001b[34m(self, atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m conv_func \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.convs:\n\u001b[32m    154\u001b[39m     atom_fea = conv_func(atom_fea, nbr_fea, nbr_fea_idx)\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m crys_fea = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpooling\u001b[49m\u001b[43m(\u001b[49m\u001b[43matom_fea\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcrystal_atom_idx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m crys_fea = \u001b[38;5;28mself\u001b[39m.conv_to_fc(\u001b[38;5;28mself\u001b[39m.conv_to_fc_softplus(crys_fea))\n\u001b[32m    157\u001b[39m crys_fea = \u001b[38;5;28mself\u001b[39m.conv_to_fc_softplus(crys_fea)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\College\\Research Paper Work\\Superconductor\\models\\CGCNN\\train\\model.py:183\u001b[39m, in \u001b[36mCrystalGraphConvNet.pooling\u001b[39m\u001b[34m(self, atom_fea, crystal_atom_idx)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpooling\u001b[39m(\u001b[38;5;28mself\u001b[39m, atom_fea, crystal_atom_idx):\n\u001b[32m    169\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[33;03m    Pooling the atom features to crystal features\u001b[39;00m\n\u001b[32m    171\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    181\u001b[39m \u001b[33;03m      Mapping from the crystal idx to atom idx\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28mlen\u001b[39m(idx_map) \u001b[38;5;28;01mfor\u001b[39;00m idx_map \u001b[38;5;129;01min\u001b[39;00m crystal_atom_idx]) ==\\\n\u001b[32m    184\u001b[39m         atom_fea.data.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    185\u001b[39m     summed_fea = [torch.mean(atom_fea[idx_map], dim=\u001b[32m0\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    186\u001b[39m                   \u001b[38;5;28;01mfor\u001b[39;00m idx_map \u001b[38;5;129;01min\u001b[39;00m crystal_atom_idx]\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(summed_fea, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mf:\\College\\Research Paper Work\\Superconductor\\models\\CGCNN\\train\\model.py:183\u001b[39m, in \u001b[36m<listcomp>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpooling\u001b[39m(\u001b[38;5;28mself\u001b[39m, atom_fea, crystal_atom_idx):\n\u001b[32m    169\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    170\u001b[39m \u001b[33;03m    Pooling the atom features to crystal features\u001b[39;00m\n\u001b[32m    171\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    181\u001b[39m \u001b[33;03m      Mapping from the crystal idx to atom idx\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28msum\u001b[39m([\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx_map \u001b[38;5;129;01min\u001b[39;00m crystal_atom_idx]) ==\\\n\u001b[32m    184\u001b[39m         atom_fea.data.shape[\u001b[32m0\u001b[39m]\n\u001b[32m    185\u001b[39m     summed_fea = [torch.mean(atom_fea[idx_map], dim=\u001b[32m0\u001b[39m, keepdim=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    186\u001b[39m                   \u001b[38;5;28;01mfor\u001b[39;00m idx_map \u001b[38;5;129;01min\u001b[39;00m crystal_atom_idx]\n\u001b[32m    187\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(summed_fea, dim=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\saman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\_tensor.py:1163\u001b[39m, in \u001b[36mTensor.__len__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1161\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(Tensor.\u001b[34m__len__\u001b[39m, (\u001b[38;5;28mself\u001b[39m,), \u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1162\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dim() == \u001b[32m0\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m1163\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mlen() of a 0-d tensor\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch._C._get_tracing_state():\n\u001b[32m   1165\u001b[39m     warnings.warn(\n\u001b[32m   1166\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mUsing len to get tensor shape might cause the trace to be incorrect. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mRecommended usage would be tensor.shape[0]. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1171\u001b[39m         stacklevel=\u001b[32m2\u001b[39m,\n\u001b[32m   1172\u001b[39m     )\n",
      "\u001b[31mTypeError\u001b[39m: len() of a 0-d tensor"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 2\n",
    "train_losses, val_losses = [], []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    \n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, all_preds, all_targets = validate(model, val_loader, criterion, device, return_preds=True)\n",
    "    \n",
    "    r2 = r2_score(all_targets, all_preds)\n",
    "    mse = mean_squared_error(all_targets, all_preds)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"RÂ² Score: {r2:.4f} | MSE: {mse:.4f} | RMSE: {rmse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9405c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_losses, label='Training Loss', marker='o')\n",
    "plt.plot(val_losses, label='Validation Loss', marker='s')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training & Validation Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cabb4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_preds, all_targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for (atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx), target, _ in test_loader:\n",
    "        atom_fea = atom_fea.to(device)\n",
    "        nbr_fea = nbr_fea.to(device)\n",
    "        nbr_fea_idx = nbr_fea_idx.to(device)\n",
    "        target = target.to(device)\n",
    "\n",
    "        output = model(atom_fea, nbr_fea, nbr_fea_idx, crystal_atom_idx)\n",
    "        all_preds.append(output.cpu().numpy())\n",
    "        all_targets.append(target.cpu().numpy())\n",
    "\n",
    "all_preds = np.concatenate(all_preds)\n",
    "all_targets = np.concatenate(all_targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212ee12b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 8))\n",
    "plt.scatter(all_targets, all_preds, alpha=0.6, edgecolors='k')\n",
    "plt.plot([all_targets.min(), all_targets.max()],\n",
    "         [all_targets.min(), all_targets.max()], 'r--', lw=2)\n",
    "plt.xlabel('True Tc')\n",
    "plt.ylabel('Predicted Tc')\n",
    "plt.title('True vs Predicted Critical Temperature (Tc)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
